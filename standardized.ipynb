{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHdGCMbQx12z4sb0Q0Qggy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayandeepmaity/vseven_lumi/blob/main/standardized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVBCfSL89NV6",
        "outputId": "e17bab22-eb4a-44fb-93d1-0f65c9676c8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t9fhTYKK89xP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, LSTM, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load the CSV data\n",
        "csv_file = \"/content/drive/MyDrive/sevenlumi_data/features.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Check non-numeric columns\n",
        "non_numeric_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
        "\n",
        "# Encode categorical columns (Gun_Type, Mic) using LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Encoding 'Gun_Type' and 'Mic' columns\n",
        "df['Gun_Type'] = encoder.fit_transform(df['Gun_Type'])\n",
        "df['Mic'] = encoder.fit_transform(df['Mic'])\n",
        "\n",
        "# Convert 'Distance' to numeric if it's in string format\n",
        "df['Distance'] = pd.to_numeric(df['Distance'], errors='coerce')  # Handle errors gracefully by setting invalid parsing as NaN\n",
        "\n",
        "# Now let's check the transformed data\n",
        "print(f\"Data types after conversion:\\n{df.dtypes}\")\n",
        "\n",
        "# Extract features (excluding 'Gunshot', 'Gun_Type', 'Distance', 'X', 'Y', 'Z')\n",
        "X_csv = df.drop(columns=['Gunshot', 'Gun_Type', 'Distance', 'X', 'Y', 'Z']).values\n",
        "\n",
        "# Normalize CSV features\n",
        "scaler = StandardScaler()\n",
        "X_csv = scaler.fit_transform(X_csv)\n",
        "\n",
        "# Check the final data shape and types\n",
        "print(f\"Shape of CSV data after transformation: {X_csv.shape}\")\n",
        "\n",
        "# Convert the normalized features back into a DataFrame\n",
        "normalized_df = pd.DataFrame(X_csv, columns=df.drop(columns=['Gunshot', 'Gun_Type', 'Distance', 'X', 'Y', 'Z']).columns)\n",
        "\n",
        "# Add the 'Gunshot' column back to the DataFrame (since it was excluded earlier)\n",
        "normalized_df['Gunshot'] = df['Gunshot']\n",
        "\n",
        "# Save the standardized data into a new CSV file\n",
        "output_csv_file = \"/content/drive/MyDrive/sevenlumi_data/standardized_features.csv\"  # Define output path for the standardized CSV\n",
        "normalized_df.to_csv(output_csv_file, index=False)\n",
        "\n",
        "print(f\"Standardized CSV saved to: {output_csv_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Od_oTYv9J2I",
        "outputId": "be9845c6-0516-4ebf-f51f-fbda852caa65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-numeric columns: ['Gun_Type', 'Distance', 'Mic']\n",
            "Data types after conversion:\n",
            "Zero_Crossing_Rate         float64\n",
            "Short_Time_Energy          float64\n",
            "RMS_Energy                 float64\n",
            "Spectral_Centroid          float64\n",
            "Spectral_RollOff           float64\n",
            "Spectral_Flux              float64\n",
            "MFCC_1                     float64\n",
            "MFCC_2                     float64\n",
            "MFCC_3                     float64\n",
            "MFCC_4                     float64\n",
            "MFCC_5                     float64\n",
            "Chroma_1                   float64\n",
            "Chroma_2                   float64\n",
            "Chroma_3                   float64\n",
            "Chroma_4                   float64\n",
            "Peak_Amplitude             float64\n",
            "Gunshot                      int64\n",
            "Gun_Type                     int64\n",
            "Distance                   float64\n",
            "X                            int64\n",
            "Y                            int64\n",
            "Z                            int64\n",
            "Mic                          int64\n",
            "Arrival_Time_Difference    float64\n",
            "dtype: object\n",
            "Shape of CSV data after transformation: (4880, 18)\n",
            "Standardized CSV saved to: /content/drive/MyDrive/sevenlumi_data/standardized_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "\n",
        "# Define directories and parameters\n",
        "image_dir = \"/content/drive/MyDrive/sevenlumi_data/feature_images\"\n",
        "output_dir = \"/content/drive/MyDrive/sevenlumi_data/standardized\"  # New folder for saving images\n",
        "image_size = (370, 370)  # Resize images to 370x370\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize lists to store image data\n",
        "X_images = []\n",
        "image_paths = []  # List to store image paths\n",
        "\n",
        "# Gather all image paths first so tqdm knows the total count\n",
        "all_image_paths = []\n",
        "for folder in os.listdir(image_dir):\n",
        "    if os.path.isdir(os.path.join(image_dir, folder)):  # For each gun type folder\n",
        "        for subfolder in os.listdir(os.path.join(image_dir, folder)):\n",
        "            if os.path.isdir(os.path.join(image_dir, folder, subfolder)):\n",
        "                for subsubfolder in os.listdir(os.path.join(image_dir, folder, subfolder)):\n",
        "                    if os.path.isdir(os.path.join(image_dir, folder, subfolder, subsubfolder)):\n",
        "                        for mic_folder in os.listdir(os.path.join(image_dir, folder, subfolder, subsubfolder)):\n",
        "                            mic_path = os.path.join(image_dir, folder, subfolder, subsubfolder, mic_folder)\n",
        "                            for file in os.listdir(mic_path):\n",
        "                                if file.endswith(\".png\"):\n",
        "                                    # Add the image path to the list\n",
        "                                    all_image_paths.append(os.path.join(mic_path, file))\n",
        "\n",
        "# Use tqdm to track the progress of image loading and standardization\n",
        "for image_path in tqdm(all_image_paths, desc=\"Standardizing and Saving Images\", unit=\"image\"):\n",
        "    # Load the image and resize to target size\n",
        "    img = load_img(image_path, target_size=image_size)\n",
        "\n",
        "    # Convert the image to an array and normalize the pixel values\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Append the image array to the list\n",
        "    X_images.append(img_array)\n",
        "\n",
        "    # Save the standardized image to the 'standardized' folder\n",
        "    # Create the output file path (keep the same folder structure inside the output directory)\n",
        "    relative_path = os.path.relpath(image_path, image_dir)  # Get the relative path of the image\n",
        "    output_image_path = os.path.join(output_dir, relative_path)\n",
        "    os.makedirs(os.path.dirname(output_image_path), exist_ok=True)  # Create subdirectories if needed\n",
        "\n",
        "    # Save the image\n",
        "    save_img(output_image_path, img_array)\n",
        "\n",
        "# Convert list of images to numpy array (if needed)\n",
        "X_images = np.array(X_images)\n",
        "\n",
        "# Check the shape of the image data\n",
        "print(f\"Shape of Image Data: {X_images.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QmlTGIgA8cS",
        "outputId": "aa133a66-3169-4ef4-e59d-2442a09d89f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Standardizing and Saving Images: 100%|██████████| 4880/4880 [28:16<00:00,  2.88image/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\n",
        "import numpy as np\n",
        "from tqdm import tqdm  # Import tqdm for progress bar\n",
        "import json  # Import json for saving and loading the checkpoint\n",
        "\n",
        "# Define directories and parameters\n",
        "image_dir = \"/content/drive/MyDrive/sevenlumi_data/feature_images\"\n",
        "output_dir = \"/content/drive/MyDrive/sevenlumi_data/standardized\"  # New folder for saving images\n",
        "checkpoint_file = \"/content/drive/MyDrive/sevenlumi_data/processed_images_checkpoint.json\"  # File to save processed images\n",
        "image_size = (370, 370)  # Resize images to 370x370\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Initialize lists to store image data\n",
        "X_images = []\n",
        "image_paths = []  # List to store image paths\n",
        "\n",
        "# Gather all image paths first so tqdm knows the total count\n",
        "all_image_paths = []\n",
        "for folder in os.listdir(image_dir):\n",
        "    if os.path.isdir(os.path.join(image_dir, folder)):  # For each gun type folder\n",
        "        for subfolder in os.listdir(os.path.join(image_dir, folder)):\n",
        "            if os.path.isdir(os.path.join(image_dir, folder, subfolder)):\n",
        "                for subsubfolder in os.listdir(os.path.join(image_dir, folder, subfolder)):\n",
        "                    if os.path.isdir(os.path.join(image_dir, folder, subfolder, subsubfolder)):\n",
        "                        for mic_folder in os.listdir(os.path.join(image_dir, folder, subfolder, subsubfolder)):\n",
        "                            mic_path = os.path.join(image_dir, folder, subfolder, subsubfolder, mic_folder)\n",
        "                            for file in os.listdir(mic_path):\n",
        "                                if file.endswith(\".png\"):\n",
        "                                    # Add the image path to the list\n",
        "                                    all_image_paths.append(os.path.join(mic_path, file))\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, \"r\") as f:\n",
        "        processed_images = json.load(f)  # Load the processed image paths from the checkpoint file\n",
        "else:\n",
        "    processed_images = []\n",
        "\n",
        "# Use tqdm to track the progress of image loading and standardization\n",
        "for image_path in tqdm(all_image_paths, desc=\"Standardizing and Saving Images\", unit=\"image\"):\n",
        "    # Skip already processed images\n",
        "    if image_path in processed_images:\n",
        "        continue\n",
        "\n",
        "    # Load the image and resize to target size\n",
        "    img = load_img(image_path, target_size=image_size)\n",
        "\n",
        "    # Convert the image to an array and normalize the pixel values\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    # Append the image array to the list\n",
        "    X_images.append(img_array)\n",
        "\n",
        "    # Save the standardized image to the 'standardized' folder\n",
        "    # Create the output file path (keep the same folder structure inside the output directory)\n",
        "    relative_path = os.path.relpath(image_path, image_dir)  # Get the relative path of the image\n",
        "    output_image_path = os.path.join(output_dir, relative_path)\n",
        "    os.makedirs(os.path.dirname(output_image_path), exist_ok=True)  # Create subdirectories if needed\n",
        "\n",
        "    # Save the image\n",
        "    save_img(output_image_path, img_array)\n",
        "\n",
        "    # Mark this image as processed\n",
        "    processed_images.append(image_path)\n",
        "\n",
        "    # Save the checkpoint to continue from where it left off\n",
        "    with open(checkpoint_file, \"w\") as f:\n",
        "        json.dump(processed_images, f)\n",
        "\n",
        "# Convert list of images to numpy array (if needed)\n",
        "X_images = np.array(X_images)\n",
        "\n",
        "# Check the shape of the image data\n",
        "print(f\"Shape of Image Data: {X_images.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElRssp4vIeRh",
        "outputId": "258f84a4-f14f-49c7-cb9e-3791e4e093e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Standardizing and Saving Images: 100%|██████████| 4880/4880 [04:28<00:00, 18.18image/s]\n"
          ]
        }
      ]
    }
  ]
}